#!/usr/bin/env python3
"""
SagaForge T2I System Smoke Test
é©—è­‰æ‰€æœ‰ T2I æ ¸å¿ƒåŠŸèƒ½æ˜¯å¦æ­£å¸¸é‹ä½œ
"""

import os
import sys
import asyncio
import logging
import time
import requests
from pathlib import Path
from typing import Dict, Any, List
import json

# Add project root to path
ROOT_DIR = Path(__file__).parent.parent
sys.path.insert(0, str(ROOT_DIR))

# Shared cache bootstrap
AI_CACHE_ROOT = os.getenv("AI_CACHE_ROOT", "../ai_warehouse/cache")
for k, v in {
    "HF_HOME": f"{AI_CACHE_ROOT}/hf",
    "TRANSFORMERS_CACHE": f"{AI_CACHE_ROOT}/hf/transformers",
    "HF_DATASETS_CACHE": f"{AI_CACHE_ROOT}/hf/datasets",
    "HUGGINGFACE_HUB_CACHE": f"{AI_CACHE_ROOT}/hf/hub",
    "TORCH_HOME": f"{AI_CACHE_ROOT}/torch",
}.items():
    os.environ[k] = v
    Path(v).mkdir(parents=True, exist_ok=True)

print(f"[cache] {AI_CACHE_ROOT}")

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class T2ISmokeTest:
    """T2I ç³»çµ±ç…™éœ§æ¸¬è©¦"""

    def __init__(self, api_base_url: str = "http://localhost:8000"):
        self.api_base_url = api_base_url
        self.test_results = {
            "config_system": False,
            "cache_system": False,
            "pipeline_loading": False,
            "safety_system": False,
            "watermark_system": False,
            "api_health": False,
            "image_generation": False,
            "lora_management": False,
            "system_status": False,
            "errors": [],
        }

    def run_all_tests(self) -> Dict[str, Any]:
        """åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦"""
        print("ğŸ§ª Starting SagaForge T2I Smoke Tests...")
        print("=" * 60)

        # Core system tests
        self.test_config_system()
        self.test_cache_system()
        self.test_pipeline_loading()
        self.test_safety_system()
        self.test_watermark_system()

        # API tests (if API is running)
        self.test_api_health()
        if self.test_results["api_health"]:
            self.test_image_generation_api()
            self.test_lora_management_api()
            self.test_system_status_api()

        # Generate report
        self.print_test_report()
        return self.test_results

    def test_config_system(self):
        """æ¸¬è©¦é…ç½®ç³»çµ±"""
        print("ğŸ”§ Testing configuration system...")

        try:
            from core.config import (
                get_settings,
                get_cache_paths,
                get_app_paths,
                bootstrap_config,
            )

            # Test settings loading
            settings = get_settings()
            assert hasattr(settings, "model"), "Settings should have model config"
            assert hasattr(settings, "api"), "Settings should have API config"

            # Test path configuration
            cache_paths = get_cache_paths()
            app_paths = get_app_paths()

            assert cache_paths.root.exists(), "Cache root should exist"
            assert app_paths.root.exists(), "App root should exist"

            # Test bootstrap
            summary = bootstrap_config(verbose=False)
            assert summary["settings_loaded"], "Settings should be loaded"

            self.test_results["config_system"] = True
            print("âœ… Configuration system test passed")

        except Exception as e:
            error_msg = f"Configuration system test failed: {e}"
            self.test_results["errors"].append(error_msg)
            print(f"âŒ {error_msg}")

    def test_cache_system(self):
        """æ¸¬è©¦å…±ç”¨å¿«å–ç³»çµ±"""
        print("ğŸ—‚ï¸ Testing shared cache system...")

        try:
            from core.shared_cache import get_shared_cache, bootstrap_cache

            # Test cache bootstrap
            cache = bootstrap_cache(verbose=False)

            # Test cache functionality
            cache_stats = cache.get_cache_stats()
            device_config = cache.get_device_config()

            assert "cache_root" in cache_stats, "Cache stats should include root"
            assert (
                "cuda_available" in device_config
            ), "Device config should include CUDA info"

            # Test model registration (with dummy data)
            test_model_path = cache.cache_paths.models / "test_model.txt"
            test_model_path.parent.mkdir(parents=True, exist_ok=True)
            test_model_path.write_text("test model content")

            success = cache.register_model(
                model_id="test_model",
                model_type="test",
                local_path=test_model_path,
                metadata={"test": True},
            )

            assert success, "Model registration should succeed"

            # Cleanup test model
            test_model_path.unlink(missing_ok=True)

            self.test_results["cache_system"] = True
            print("âœ… Shared cache system test passed")

        except Exception as e:
            error_msg = f"Cache system test failed: {e}"
            self.test_results["errors"].append(error_msg)
            print(f"âŒ {error_msg}")

    def test_pipeline_loading(self):
        """æ¸¬è©¦ Pipeline è¼‰å…¥"""
        print("ğŸ¨ Testing T2I pipeline loading...")

        try:
            from core.t2i.pipeline import get_pipeline_manager

            # Get pipeline manager
            manager = get_pipeline_manager()

            # Test pipeline creation
            pipeline = manager.get_pipeline("sd15")
            assert pipeline is not None, "Pipeline should be created"

            # Test pipeline status
            status = pipeline.get_status()
            assert "model_type" in status, "Status should include model type"
            assert status["model_type"] == "sd15", "Model type should match"

            # Note: We don't actually load the pipeline to avoid downloading models
            # In a real test environment, you would test pipeline.load_pipeline()

            self.test_results["pipeline_loading"] = True
            print("âœ… Pipeline loading test passed")

        except Exception as e:
            error_msg = f"Pipeline loading test failed: {e}"
            self.test_results["errors"].append(error_msg)
            print(f"âŒ {error_msg}")

    def test_safety_system(self):
        """æ¸¬è©¦å®‰å…¨ç³»çµ±"""
        print("ğŸ›¡ï¸ Testing safety system...")

        try:
            from core.t2i.safety import get_safety_checker, test_safety_system

            # Run built-in safety test
            test_results = asyncio.run(test_safety_system())

            # Check basic functionality
            assert isinstance(test_results, dict), "Safety test should return dict"

            # Test individual components
            checker = asyncio.run(get_safety_checker())
            status = checker.get_status()

            assert "enabled" in status, "Safety status should include enabled flag"

            self.test_results["safety_system"] = True
            print("âœ… Safety system test passed")

        except Exception as e:
            error_msg = f"Safety system test failed: {e}"
            self.test_results["errors"].append(error_msg)
            print(f"âŒ {error_msg}")

    def test_watermark_system(self):
        """æ¸¬è©¦æµ®æ°´å°ç³»çµ±"""
        print("ğŸ’§ Testing watermark system...")

        try:
            from core.t2i.watermark import get_watermark_manager, test_watermark_system

            # Run built-in watermark test
            test_results = test_watermark_system()

            # Check basic functionality
            assert isinstance(test_results, dict), "Watermark test should return dict"
            assert (
                len(test_results["errors"]) == 0
            ), f"Watermark test should have no errors: {test_results['errors']}"

            # Test manager
            manager = get_watermark_manager()
            status = manager.get_watermark_status()

            assert "enabled" in status, "Watermark status should include enabled flag"

            self.test_results["watermark_system"] = True
            print("âœ… Watermark system test passed")

        except Exception as e:
            error_msg = f"Watermark system test failed: {e}"
            self.test_results["errors"].append(error_msg)
            print(f"âŒ {error_msg}")

    def test_api_health(self):
        """æ¸¬è©¦ API å¥åº·ç‹€æ…‹"""
        print("ğŸŒ Testing API health...")

        try:
            response = requests.get(f"{self.api_base_url}/healthz", timeout=10)

            if response.status_code == 200:
                health_data = response.json()
                assert "status" in health_data, "Health response should include status"

                self.test_results["api_health"] = True
                print("âœ… API health test passed")
            else:
                error_msg = f"API health check failed: HTTP {response.status_code}"
                self.test_results["errors"].append(error_msg)
                print(f"âŒ {error_msg}")

        except requests.RequestException as e:
            error_msg = f"API health test failed (connection): {e}"
            self.test_results["errors"].append(error_msg)
            print(f"âŒ {error_msg}")
        except Exception as e:
            error_msg = f"API health test failed: {e}"
            self.test_results["errors"].append(error_msg)
            print(f"âŒ {error_msg}")

    def test_image_generation_api(self):
        """æ¸¬è©¦åœ–åƒç”Ÿæˆ API"""
        print("ğŸ¨ Testing image generation API...")

        try:
            # Test generation request
            generation_request = {
                "prompt": "A simple test image",
                "model_type": "sd15",
                "width": 256,
                "height": 256,
                "num_inference_steps": 5,  # Very fast for testing
                "num_images": 1,
            }

            response = requests.post(
                f"{self.api_base_url}/t2i/generate", json=generation_request, timeout=60
            )

            if response.status_code == 200:
                result = response.json()
                assert "job_id" in result, "Generation response should include job_id"
                assert "status" in result, "Generation response should include status"

                self.test_results["image_generation"] = True
                print("âœ… Image generation API test passed")
            else:
                error_msg = f"Image generation API failed: HTTP {response.status_code}"
                if response.text:
                    error_msg += f" - {response.text[:200]}"
                self.test_results["errors"].append(error_msg)
                print(f"âŒ {error_msg}")

        except Exception as e:
            error_msg = f"Image generation API test failed: {e}"
            self.test_results["errors"].append(error_msg)
            print(f"âŒ {error_msg}")

    def test_lora_management_api(self):
        """æ¸¬è©¦ LoRA ç®¡ç† API"""
        print("ğŸ”„ Testing LoRA management API...")

        try:
            # Test LoRA status (should work even without LoRAs loaded)
            response = requests.get(f"{self.api_base_url}/t2i/models", timeout=10)

            if response.status_code == 200:
                models = response.json()
                assert isinstance(models, dict), "Models response should be a dict"
                assert "lora" in models, "Models should include LoRA category"

                self.test_results["lora_management"] = True
                print("âœ… LoRA management API test passed")
            else:
                error_msg = f"LoRA management API failed: HTTP {response.status_code}"
                self.test_results["errors"].append(error_msg)
                print(f"âŒ {error_msg}")

        except Exception as e:
            error_msg = f"LoRA management API test failed: {e}"
            self.test_results["errors"].append(error_msg)
            print(f"âŒ {error_msg}")

    def test_system_status_api(self):
        """æ¸¬è©¦ç³»çµ±ç‹€æ…‹ API"""
        print("ğŸ“Š Testing system status API...")

        try:
            response = requests.get(
                f"{self.api_base_url}/t2i/system/status", timeout=10
            )

            if response.status_code == 200:
                status = response.json()
                assert "status" in status, "System status should include status field"
                assert "device" in status, "System status should include device info"

                self.test_results["system_status"] = True
                print("âœ… System status API test passed")
            else:
                error_msg = f"System status API failed: HTTP {response.status_code}"
                self.test_results["errors"].append(error_msg)
                print(f"âŒ {error_msg}")

        except Exception as e:
            error_msg = f"System status API test failed: {e}"
            self.test_results["errors"].append(error_msg)
            print(f"âŒ {error_msg}")

    def print_test_report(self):
        """æ‰“å°æ¸¬è©¦å ±å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ§ª SagaForge T2I Smoke Test Report")
        print("=" * 60)

        total_tests = len([k for k in self.test_results.keys() if k != "errors"])
        passed_tests = sum(
            [1 for k, v in self.test_results.items() if k != "errors" and v]
        )

        print(f"ğŸ“ˆ Test Results: {passed_tests}/{total_tests} passed")
        print()

        # Print individual test results
        for test_name, result in self.test_results.items():
            if test_name == "errors":
                continue

            status_icon = "âœ…" if result else "âŒ"
            formatted_name = test_name.replace("_", " ").title()
            print(f"{status_icon} {formatted_name}")

        # Print errors if any
        if self.test_results["errors"]:
            print(f"\nâŒ Errors ({len(self.test_results['errors'])}):")
            for i, error in enumerate(self.test_results["errors"], 1):
                print(f"  {i}. {error}")

        # Overall status
        print(f"\nğŸ¯ Overall Status: ", end="")
        if passed_tests == total_tests:
            print("ğŸŸ¢ ALL TESTS PASSED - System is ready!")
        elif passed_tests >= total_tests * 0.8:
            print("ğŸŸ¡ MOST TESTS PASSED - System is mostly functional")
        elif passed_tests >= total_tests * 0.5:
            print("ğŸŸ  SOME TESTS FAILED - System has issues")
        else:
            print("ğŸ”´ MANY TESTS FAILED - System needs attention")

        print("=" * 60)


def main():
    """ä¸»å‡½æ•¸"""
    import argparse

    parser = argparse.ArgumentParser(description="SagaForge T2I System Smoke Test")
    parser.add_argument(
        "--api-url",
        default="http://localhost:8000",
        help="API base URL (default: http://localhost:8000)",
    )
    parser.add_argument(
        "--skip-api",
        action="store_true",
        help="Skip API tests (test only core components)",
    )
    parser.add_argument("--save-report", help="Save test report to JSON file")

    args = parser.parse_args()

    # Run tests
    tester = T2ISmokeTest(api_base_url=args.api_url)

    if args.skip_api:
        # Override API tests to skip them
        tester.test_results["api_health"] = (
            True  # Mark as passed to skip dependent tests
        )
        print("â­ï¸ Skipping API tests as requested")

    results = tester.run_all_tests()

    # Save report if requested
    if args.save_report:
        report_path = Path(args.save_report)
        report_path.parent.mkdir(parents=True, exist_ok=True)

        with open(report_path, "w") as f:
            json.dump(results, f, indent=2)

        print(f"ğŸ“„ Test report saved to: {report_path}")

    # Exit with appropriate code
    total_tests = len([k for k in results.keys() if k != "errors"])
    passed_tests = sum([1 for k, v in results.items() if k != "errors" and v])

    if passed_tests == total_tests:
        return 0  # All tests passed
    elif passed_tests >= total_tests * 0.8:
        return 1  # Most tests passed
    else:
        return 2  # Too many failures


def test_core_components_only():
    """åªæ¸¬è©¦æ ¸å¿ƒçµ„ä»¶ï¼ˆä¸éœ€è¦ API é‹è¡Œï¼‰"""
    print("ğŸ§ª Testing Core Components Only...")

    tester = T2ISmokeTest()

    # Run only core tests
    tester.test_config_system()
    tester.test_cache_system()
    tester.test_pipeline_loading()
    tester.test_safety_system()
    tester.test_watermark_system()

    # Set API tests as skipped
    tester.test_results["api_health"] = True
    tester.test_results["image_generation"] = True
    tester.test_results["lora_management"] = True
    tester.test_results["system_status"] = True

    tester.print_test_report()
    return tester.test_results


def test_installation():
    """æ¸¬è©¦å®‰è£ç’°å¢ƒ"""
    print("ğŸ“¦ Testing Installation Environment...")
    print("=" * 50)

    # Test Python version
    print(f"ğŸ Python Version: {sys.version}")

    # Test required packages
    required_packages = [
        "torch",
        "diffusers",
        "transformers",
        "PIL",
        "fastapi",
        "pydantic",
        "numpy",
        "requests",
    ]

    missing_packages = []

    for package in required_packages:
        try:
            __import__(package)
            print(f"âœ… {package}")
        except ImportError:
            print(f"âŒ {package} - MISSING")
            missing_packages.append(package)

    # Test CUDA availability
    try:
        import torch

        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            print(f"âœ… CUDA Available: {gpu_name} ({gpu_memory:.1f}GB)")
        else:
            print("âš ï¸ CUDA Not Available - CPU mode only")
    except ImportError:
        print("âŒ PyTorch not available")

    # Test cache directories
    cache_paths = [
        AI_CACHE_ROOT,
        f"{AI_CACHE_ROOT}/hf",
        f"{AI_CACHE_ROOT}/torch",
        f"{AI_CACHE_ROOT}/models",
        f"{AI_CACHE_ROOT}/datasets",
        f"{AI_CACHE_ROOT}/outputs",
    ]

    print(f"\nğŸ“ Cache Directories:")
    for path in cache_paths:
        path_obj = Path(path)
        if path_obj.exists():
            print(f"âœ… {path}")
        else:
            print(f"âŒ {path} - Will be created")
            path_obj.mkdir(parents=True, exist_ok=True)

    # Summary
    print(f"\nğŸ¯ Installation Status:")
    if missing_packages:
        print(f"âŒ Missing packages: {', '.join(missing_packages)}")
        print("ğŸ“¦ Install missing packages with:")
        print(f"   pip install {' '.join(missing_packages)}")
        return False
    else:
        print("âœ… All required packages installed")
        return True


if __name__ == "__main__":
    # Quick installation test first
    print("ğŸš€ SagaForge T2I System Smoke Test")
    print("=" * 60)

    installation_ok = test_installation()

    if not installation_ok:
        print("\nâŒ Installation issues detected. Please fix before running tests.")
        sys.exit(3)

    print("\n")

    # Run main tests
    sys.exit(main())


# ===== Additional Utility Functions =====


def quick_api_test(api_url: str = "http://localhost:8000") -> bool:
    """å¿«é€Ÿ API æ¸¬è©¦"""
    try:
        response = requests.get(f"{api_url}/healthz", timeout=5)
        return response.status_code == 200
    except:
        return False


def test_gpu_memory():
    """æ¸¬è©¦ GPU è¨˜æ†¶é«”"""
    try:
        import torch

        if not torch.cuda.is_available():
            print("âŒ CUDA not available")
            return False

        # Test GPU memory allocation
        device = torch.device("cuda")

        # Allocate a small tensor
        test_tensor = torch.randn(100, 100, device=device)

        allocated = torch.cuda.memory_allocated() / (1024**2)  # MB
        total = torch.cuda.get_device_properties(0).total_memory / (1024**2)  # MB

        print(f"âœ… GPU Memory Test: {allocated:.1f}MB allocated / {total:.1f}MB total")

        # Clean up
        del test_tensor
        torch.cuda.empty_cache()

        return True

    except Exception as e:
        print(f"âŒ GPU Memory Test Failed: {e}")
        return False


def benchmark_generation_speed():
    """åŸºæº–æ¸¬è©¦ç”Ÿæˆé€Ÿåº¦"""
    try:
        from core.t2i.pipeline import get_pipeline_manager
        import time

        print("â±ï¸ Running generation speed benchmark...")

        manager = get_pipeline_manager()
        pipeline = manager.get_pipeline("sd15")

        # Note: This would require loading actual models
        # For smoke test, we just test the pipeline creation

        start_time = time.time()
        # pipeline.load_pipeline()  # Would actually load model
        setup_time = time.time() - start_time

        print(f"âœ… Pipeline setup time: {setup_time:.2f}s")
        print("âš ï¸ Actual generation benchmark requires loaded models")

        return True

    except Exception as e:
        print(f"âŒ Benchmark failed: {e}")
        return False


# ===== Example Usage =====
"""
ä½¿ç”¨ç¯„ä¾‹ï¼š

# åŸºæœ¬æ¸¬è©¦ï¼ˆåŒ…å« APIï¼‰
python scripts/smoke_test_t2i.py

# åªæ¸¬è©¦æ ¸å¿ƒçµ„ä»¶
python scripts/smoke_test_t2i.py --skip-api

# ä½¿ç”¨è‡ªå®šç¾© API URL
python scripts/smoke_test_t2i.py --api-url http://localhost:8080

# å„²å­˜æ¸¬è©¦å ±å‘Š
python scripts/smoke_test_t2i.py --save-report test_report.json

# åªåœ¨ä»£ç¢¼ä¸­æ¸¬è©¦æ ¸å¿ƒçµ„ä»¶
if __name__ == "__main__":
    from scripts.smoke_test_t2i import test_core_components_only
    results = test_core_components_only()
"""
