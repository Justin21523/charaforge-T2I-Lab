# ===== workers/utils/task_progress.py =====
"""
TaskProgress utility class for Celery task progress tracking
çµ±ä¸€çš„ä»»å‹™é€²åº¦è¿½è¹¤ä»‹é¢
"""

import logging
from typing import Dict, Any, Optional
from datetime import datetime
from celery import current_task

logger = logging.getLogger(__name__)


class TaskProgress:
    """Celery ä»»å‹™é€²åº¦è¿½è¹¤å™¨"""

    def __init__(self, task_instance, total_steps: int = 100):
        self.task = task_instance
        self.total_steps = total_steps
        self.current_step = 0
        self.start_time = datetime.now()
        self.last_update = self.start_time

    def update(self, step: int, message: str, extra_data: Optional[Dict] = None):
        """æ›´æ–°ä»»å‹™é€²åº¦"""
        self.current_step = step
        self.last_update = datetime.now()

        elapsed = (self.last_update - self.start_time).total_seconds()

        state_data = {
            "current": step,
            "total": self.total_steps,
            "message": message,
            "elapsed_seconds": elapsed,
            "timestamp": self.last_update.isoformat(),
        }

        if extra_data:
            state_data.update(extra_data)

        self.task.update_state(state="PROGRESS", meta=state_data)

        logger.info(f"[{self.task.request.id}] {step}/{self.total_steps}: {message}")

    def complete(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """æ¨™è¨˜ä»»å‹™å®Œæˆ"""
        elapsed = (datetime.now() - self.start_time).total_seconds()

        final_result = {
            "status": "SUCCESS",
            "result": result,
            "elapsed_seconds": elapsed,
            "completed_at": datetime.now().isoformat(),
        }

        logger.info(f"[{self.task.request.id}] Task completed in {elapsed:.2f}s")
        return final_result

    def fail(self, error_message: str) -> Dict[str, Any]:
        """æ¨™è¨˜ä»»å‹™å¤±æ•—"""
        elapsed = (datetime.now() - self.start_time).total_seconds()

        error_result = {
            "status": "FAILURE",
            "error": error_message,
            "elapsed_seconds": elapsed,
            "failed_at": datetime.now().isoformat(),
        }

        logger.error(f"[{self.task.request.id}] Task failed: {error_message}")
        return error_result


# ===== workers/utils/gpu_manager.py =====
"""
GPU resource management for Celery workers
GPU è³‡æºç®¡ç†èˆ‡ä½è¨˜æ†¶é«”æœ€ä½³åŒ–
"""

import os
import gc
import torch
import psutil
import logging
from typing import Optional, Dict
from contextlib import contextmanager

logger = logging.getLogger(__name__)


class GPUManager:
    """GPU è³‡æºç®¡ç†å™¨"""

    def __init__(self, low_vram_mode: bool = True):
        self.low_vram_mode = low_vram_mode
        self.device = self._detect_device()
        self.initial_memory = self._get_memory_info()

    def _detect_device(self) -> str:
        """åµæ¸¬å¯ç”¨è£ç½®"""
        if torch.cuda.is_available():
            device = f"cuda:{torch.cuda.current_device()}"
            gpu_name = torch.cuda.get_device_name()
            memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9

            logger.info(f"ğŸš€ Using GPU: {gpu_name} ({memory_gb:.1f}GB)")
            return device
        else:
            logger.warning("âš ï¸ CUDA not available, using CPU")
            return "cpu"

    def _get_memory_info(self) -> Dict:
        """å–å¾—è¨˜æ†¶é«”è³‡è¨Š"""
        info = {
            "cpu_percent": psutil.virtual_memory().percent,
            "cpu_available_gb": psutil.virtual_memory().available / 1e9,
        }

        if torch.cuda.is_available():
            info.update(
                {
                    "gpu_allocated_gb": torch.cuda.memory_allocated() / 1e9,
                    "gpu_reserved_gb": torch.cuda.memory_reserved() / 1e9,
                    "gpu_total_gb": torch.cuda.get_device_properties(0).total_memory
                    / 1e9,
                }
            )

        return info

    def setup_for_inference(self):
        """è¨­å®šæ¨ç†æ¨¡å¼"""
        if self.device.startswith("cuda"):
            torch.cuda.empty_cache()

            if self.low_vram_mode:
                # Enable memory efficient attention
                os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:512"
                logger.info("ğŸ”§ Low VRAM mode enabled")

    def setup_for_training(self):
        """è¨­å®šè¨“ç·´æ¨¡å¼"""
        if self.device.startswith("cuda"):
            torch.cuda.empty_cache()
            gc.collect()

            # Set memory fraction for training
            if self.low_vram_mode:
                torch.cuda.set_per_process_memory_fraction(0.8)
                logger.info("ğŸ”§ Training memory fraction set to 80%")

    def cleanup(self):
        """æ¸…ç† GPU è¨˜æ†¶é«”"""
        if self.device.startswith("cuda"):
            torch.cuda.empty_cache()
            gc.collect()

        final_memory = self._get_memory_info()
        logger.info(f"ğŸ§¹ Memory cleanup completed: {final_memory}")

    @contextmanager
    def inference_context(self):
        """æ¨ç†ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        try:
            self.setup_for_inference()
            yield self.device
        finally:
            self.cleanup()

    @contextmanager
    def training_context(self):
        """è¨“ç·´ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        try:
            self.setup_for_training()
            yield self.device
        finally:
            self.cleanup()


# ===== workers/utils/job_tracker.py (Enhanced) =====
"""
Enhanced job tracking with database integration
å¢å¼·çš„å·¥ä½œè¿½è¹¤ç³»çµ±
"""

import json
import redis
from typing import Dict, Any, Optional, List
from datetime import datetime, timedelta
from enum import Enum
from pydantic import BaseModel


# Job status enumeration
class JobStatus(str, Enum):
    PENDING = "PENDING"
    STARTED = "STARTED"
    PROGRESS = "PROGRESS"
    SUCCESS = "SUCCESS"
    FAILURE = "FAILURE"
    RETRY = "RETRY"
    REVOKED = "REVOKED"


class JobInfo(BaseModel):
    """Job information model"""

    job_id: str
    task_name: str
    status: JobStatus
    progress: int = 0
    total_steps: int = 100
    message: str = ""
    created_at: datetime
    updated_at: datetime
    error: Optional[str] = None
    result: Optional[Dict[str, Any]] = None
    metadata: Dict[str, Any] = {}


class JobTracker:
    """å¢å¼·çš„å·¥ä½œè¿½è¹¤å™¨"""

    def __init__(self, redis_url: str = "redis://localhost:6379/0"):
        self.redis_client = redis.from_url(redis_url)
        self.key_prefix = "job:"
        self.expiry_seconds = 86400 * 7  # 7 days

    def _make_key(self, job_id: str) -> str:
        return f"{self.key_prefix}{job_id}"

    def create_job(self, job_id: str, task_name: str, metadata: Dict = None) -> JobInfo:
        """å»ºç«‹æ–°å·¥ä½œè¨˜éŒ„"""
        job_info = JobInfo(
            job_id=job_id,
            task_name=task_name,
            status=JobStatus.PENDING,
            created_at=datetime.now(),
            updated_at=datetime.now(),
            metadata=metadata or {},
        )

        self._save_job(job_info)
        return job_info

    def update_job(self, job_id: str, **updates) -> Optional[JobInfo]:
        """æ›´æ–°å·¥ä½œç‹€æ…‹"""
        job_info = self.get_job(job_id)
        if not job_info:
            return None

        # Update fields
        for key, value in updates.items():
            if hasattr(job_info, key):
                setattr(job_info, key, value)

        job_info.updated_at = datetime.now()
        self._save_job(job_info)
        return job_info

    def get_job(self, job_id: str) -> Optional[JobInfo]:
        """å–å¾—å·¥ä½œè³‡è¨Š"""
        try:
            key = self._make_key(job_id)
            data = self.redis_client.get(key)

            if data:
                job_dict = json.loads(data)
                return JobInfo(**job_dict)

        except Exception as e:
            logger.error(f"Failed to get job {job_id}: {e}")

        return None

    def _save_job(self, job_info: JobInfo):
        """å„²å­˜å·¥ä½œè³‡è¨Š"""
        try:
            key = self._make_key(job_info.job_id)
            data = job_info.model_dump_json()
            self.redis_client.setex(key, self.expiry_seconds, data)

        except Exception as e:
            logger.error(f"Failed to save job {job_info.job_id}: {e}")

    def list_jobs(self, limit: int = 50) -> List[JobInfo]:
        """åˆ—å‡ºæ‰€æœ‰å·¥ä½œ"""
        try:
            pattern = f"{self.key_prefix}*"
            keys = self.redis_client.keys(pattern)

            jobs = []
            for key in keys[:limit]:
                data = self.redis_client.get(key)
                if data:
                    job_dict = json.loads(data)
                    jobs.append(JobInfo(**job_dict))

            # Sort by created_at desc
            jobs.sort(key=lambda x: x.created_at, reverse=True)
            return jobs

        except Exception as e:
            logger.error(f"Failed to list jobs: {e}")
            return []


# ===== workers/tasks/training.py (Fixed) =====
"""
Fixed LoRA Training Tasks with proper imports and error handling
ä¿®å¾©çš„ LoRA è¨“ç·´ä»»å‹™
"""

import os
import json
import logging
import time
import shutil
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime

import torch
from celery import current_task

# Fixed imports
from workers.celery_app import celery_app
from workers.utils.task_progress import TaskProgress
from workers.utils.gpu_manager import GPUManager
from workers.utils.job_tracker import JobTracker, JobStatus
from core.config import get_app_paths

logger = logging.getLogger(__name__)

# Initialize job tracker
job_tracker = JobTracker()


@celery_app.task(bind=True, name="workers.tasks.training.train_lora")
def train_lora(self, config: Dict[str, Any]) -> Dict[str, Any]:
    """
    LoRA è¨“ç·´ä»»å‹™ - å®Œæ•´å¯¦ä½œ
    """
    progress = TaskProgress(self, total_steps=100)
    gpu_manager = GPUManager(low_vram_mode=config.get("low_vram_mode", True))

    # Create job tracking
    job_id = self.request.id
    job_tracker.create_job(job_id, "train_lora", config)

    try:
        logger.info(
            f"ğŸ¯ Starting LoRA training: {config.get('project_name', 'unnamed')}"
        )

        # Update job status
        job_tracker.update_job(job_id, status=JobStatus.STARTED)

        # Step 1: Environment setup (10%)
        progress.update(5, "Setting up training environment...")

        app_paths = get_app_paths()
        project_name = config.get("project_name", f"lora_{int(time.time())}")
        project_dir = app_paths.training_runs / project_name
        project_dir.mkdir(parents=True, exist_ok=True)

        # Save config
        config_file = project_dir / "training_config.json"
        with open(config_file, "w") as f:
            json.dump(config, f, indent=2, default=str)

        progress.update(10, "Environment setup completed")

        # Step 2: GPU setup and model loading (20%)
        with gpu_manager.training_context() as device:
            progress.update(15, f"GPU setup completed - Device: {device}")

            # Step 3: Dataset preparation (30%)
            progress.update(20, "Loading training dataset...")

            dataset_info = prepare_lora_dataset(config, project_dir)
            if not dataset_info["valid"]:
                raise ValueError(f"Dataset validation failed: {dataset_info['errors']}")

            progress.update(
                30, f"Dataset ready: {dataset_info['sample_count']} samples"
            )

            # Step 4: Model initialization (40%)
            progress.update(35, "Initializing LoRA model...")

            model_info = initialize_lora_model(config, device)
            progress.update(40, f"Model loaded: {model_info['base_model']}")

            # Step 5: Training loop (40% -> 85%)
            progress.update(45, "Starting training...")

            training_result = run_lora_training(
                config,
                project_dir,
                device,
                progress_callback=lambda step, msg: progress.update(
                    45 + int(step * 0.4), msg
                ),
            )

            progress.update(85, "Training completed")

            # Step 6: Model export (95%)
            progress.update(90, "Exporting trained model...")

            export_result = export_lora_model(training_result, project_dir)
            progress.update(95, "Model export completed")

        # Final result
        final_result = {
            "project_name": project_name,
            "project_dir": str(project_dir),
            "training_stats": training_result.get("stats", {}),
            "model_path": export_result["model_path"],
            "sample_images": export_result.get("sample_images", []),
            "config": config,
        }

        # Update job status
        job_tracker.update_job(
            job_id, status=JobStatus.SUCCESS, progress=100, result=final_result
        )

        return progress.complete(final_result)

    except Exception as e:
        error_msg = f"LoRA training failed: {str(e)}"
        logger.error(error_msg, exc_info=True)

        # Update job status
        job_tracker.update_job(job_id, status=JobStatus.FAILURE, error=error_msg)

        return progress.fail(error_msg)


def prepare_lora_dataset(config: Dict[str, Any], project_dir: Path) -> Dict[str, Any]:
    """æº–å‚™ LoRA è¨“ç·´æ•¸æ“šé›†"""
    try:
        dataset_path = config.get("dataset_path")
        if not dataset_path:
            raise ValueError("Dataset path not specified")

        dataset_path = Path(dataset_path)
        if not dataset_path.exists():
            raise ValueError(f"Dataset path does not exist: {dataset_path}")

        # Simple validation - count image files
        image_extensions = {".jpg", ".jpeg", ".png", ".webp"}
        image_files = []

        for ext in image_extensions:
            image_files.extend(dataset_path.glob(f"*{ext}"))
            image_files.extend(dataset_path.glob(f"*{ext.upper()}"))

        if len(image_files) < 5:
            raise ValueError(
                f"Insufficient training images: {len(image_files)} (minimum 5)"
            )

        # Copy dataset to project directory
        project_dataset_dir = project_dir / "dataset"
        project_dataset_dir.mkdir(exist_ok=True)

        # Create metadata file
        metadata = {
            "source_path": str(dataset_path),
            "image_count": len(image_files),
            "prepared_at": datetime.now().isoformat(),
            "config": config,
        }

        with open(project_dataset_dir / "metadata.json", "w") as f:
            json.dump(metadata, f, indent=2)

        return {
            "valid": True,
            "sample_count": len(image_files),
            "dataset_dir": str(project_dataset_dir),
            "metadata": metadata,
        }

    except Exception as e:
        return {"valid": False, "errors": [str(e)]}


def initialize_lora_model(config: Dict[str, Any], device: str) -> Dict[str, Any]:
    """åˆå§‹åŒ– LoRA æ¨¡å‹"""
    try:
        base_model = config.get("base_model", "runwayml/stable-diffusion-v1-5")

        # For now, return mock initialization
        # In real implementation, this would load diffusers pipeline
        model_info = {
            "base_model": base_model,
            "device": device,
            "precision": "fp16" if device.startswith("cuda") else "fp32",
            "low_vram_mode": config.get("low_vram_mode", True),
        }

        logger.info(f"Model initialized: {model_info}")
        return model_info

    except Exception as e:
        raise RuntimeError(f"Failed to initialize model: {e}")


def run_lora_training(
    config: Dict[str, Any], project_dir: Path, device: str, progress_callback=None
) -> Dict[str, Any]:
    """åŸ·è¡Œ LoRA è¨“ç·´"""
    try:
        # Mock training process for now
        # In real implementation, this would run the actual LoRA training

        total_steps = config.get("max_train_steps", 1000)

        for step in range(0, total_steps, 50):
            if progress_callback:
                progress_pct = (step / total_steps) * 100
                progress_callback(progress_pct, f"Training step {step}/{total_steps}")

            # Simulate training time
            time.sleep(0.1)

        training_stats = {
            "total_steps": total_steps,
            "final_loss": 0.023,
            "best_loss": 0.019,
            "training_time_minutes": 45.5,
        }

        return {
            "status": "completed",
            "stats": training_stats,
            "model_files": ["lora_weights.safetensors"],
        }

    except Exception as e:
        raise RuntimeError(f"Training failed: {e}")


def export_lora_model(
    training_result: Dict[str, Any], project_dir: Path
) -> Dict[str, Any]:
    """åŒ¯å‡ºè¨“ç·´å®Œæˆçš„ LoRA æ¨¡å‹"""
    try:
        export_dir = project_dir / "exports"
        export_dir.mkdir(exist_ok=True)

        # Create export metadata
        export_info = {
            "exported_at": datetime.now().isoformat(),
            "training_stats": training_result.get("stats", {}),
            "files": training_result.get("model_files", []),
        }

        with open(export_dir / "export_info.json", "w") as f:
            json.dump(export_info, f, indent=2)

        return {
            "model_path": str(export_dir),
            "files": training_result.get("model_files", []),
            "export_info": export_info,
        }

    except Exception as e:
        raise RuntimeError(f"Model export failed: {e}")


# ===== workers/tasks/generation.py (Enhanced) =====
"""
Enhanced generation tasks with proper GPU management
å¢å¼·çš„åœ–ç‰‡ç”Ÿæˆä»»å‹™
"""

import os
import time
from pathlib import Path
from typing import Dict, Any, Optional

from workers.celery_app import celery_app
from workers.utils.task_progress import TaskProgress
from workers.utils.gpu_manager import GPUManager
from core.config import get_app_paths

logger = logging.getLogger(__name__)


@celery_app.task(bind=True, name="workers.tasks.generation.generate_image")
def generate_image_task(self, params: Dict[str, Any]) -> Dict[str, Any]:
    """
    åœ–ç‰‡ç”Ÿæˆä»»å‹™ - æ”¯æ´ LoRA å’Œä½è¨˜æ†¶é«”æ¨¡å¼
    """
    progress = TaskProgress(self, total_steps=100)
    gpu_manager = GPUManager(low_vram_mode=params.get("low_vram_mode", True))

    try:
        progress.update(10, "Initializing image generation...")

        # Setup output directory
        app_paths = get_app_paths()
        output_dir = app_paths.outputs / "generations"
        output_dir.mkdir(parents=True, exist_ok=True)

        progress.update(20, "Setting up GPU environment...")

        with gpu_manager.inference_context() as device:
            progress.update(30, f"GPU ready: {device}")

            # Mock generation process
            prompt = params.get("prompt", "a beautiful landscape")
            steps = params.get("steps", 20)

            progress.update(50, f"Generating: {prompt[:50]}...")

            # Simulate generation time
            for i in range(steps):
                time.sleep(0.1)  # Simulate processing
                step_progress = 50 + int((i / steps) * 40)
                progress.update(step_progress, f"Generation step {i+1}/{steps}")

            progress.update(90, "Saving generated image...")

            # Mock save process
            timestamp = int(time.time())
            filename = f"generated_{timestamp}.png"
            output_path = output_dir / filename

            # Create dummy file for now
            output_path.write_text(f"Mock image: {prompt}")

            result = {
                "image_path": str(output_path),
                "filename": filename,
                "prompt": prompt,
                "params": params,
                "device": device,
                "generation_time_seconds": steps * 0.1,
            }

            progress.update(100, "Generation completed")
            return progress.complete(result)

    except Exception as e:
        error_msg = f"Image generation failed: {str(e)}"
        logger.error(error_msg, exc_info=True)
        return progress.fail(error_msg)


# ===== workers/celery_app.py (Fixed) =====
"""
Fixed Celery app configuration with proper imports
ä¿®å¾©çš„ Celery æ‡‰ç”¨é…ç½®
"""

import os
import sys
import logging
from pathlib import Path
from celery import Celery
from celery.signals import worker_init, worker_shutdown

# Add project root to path
ROOT_DIR = Path(__file__).parent.parent
sys.path.insert(0, str(ROOT_DIR))

# Bootstrap configuration first
from core.shared_cache import bootstrap_cache
from core.config import bootstrap_config, get_settings

logger = logging.getLogger(__name__)

# Initialize configuration
try:
    bootstrap_config(verbose=False)
    bootstrap_cache(verbose=False)
    settings = get_settings()
    logger.info("âœ… Celery configuration initialized")
except Exception as e:
    logger.error(f"âŒ Failed to initialize Celery configuration: {e}")

    # Use fallback settings
    class FallbackSettings:
        class celery:
            broker_url = "redis://localhost:6379/0"
            result_backend = "redis://localhost:6379/0"
            worker_concurrency = 1
            worker_prefetch_multiplier = 1
            task_soft_time_limit = 3600
            task_time_limit = 7200

    settings = FallbackSettings()

# Create Celery app
celery_app = Celery(
    "sagaforge_workers",
    broker=settings.celery.broker_url,
    backend=settings.celery.result_backend,
    include=[
        "workers.tasks.training",
        "workers.tasks.generation",
        "workers.tasks.batch",
    ],
)

# Configure Celery
celery_app.conf.update(
    # Task routing
    task_routes={
        "workers.tasks.training.*": {"queue": "training"},
        "workers.tasks.generation.*": {"queue": "generation"},
        "workers.tasks.batch.*": {"queue": "batch"},
    },
    # Worker settings
    worker_prefetch_multiplier=getattr(
        settings.celery, "worker_prefetch_multiplier", 1
    ),
    task_acks_late=True,
    worker_disable_rate_limits=True,
    # Time limits
    task_soft_time_limit=getattr(settings.celery, "task_soft_time_limit", 3600),
    task_time_limit=getattr(settings.celery, "task_time_limit", 7200),
    # Result settings
    result_expires=7200,  # 2 hours
    result_compression="gzip",
    # Serialization
    task_serializer="json",
    result_serializer="json",
    accept_content=["json"],
    timezone="UTC",
    enable_utc=True,
    # Error handling
    task_reject_on_worker_lost=True,
    task_ignore_result=False,
    # Performance
    worker_pool="solo" if os.name == "nt" else "prefork",
    worker_concurrency=getattr(settings.celery, "worker_concurrency", 1),
    # Monitoring
    worker_send_task_events=True,
    task_send_sent_event=True,
)


@worker_init.connect
def worker_init_handler(sender=None, conf=None, **kwargs):
    """Worker åˆå§‹åŒ–è™•ç†"""
    try:
        logger.info("ğŸš€ Initializing Celery worker...")

        # Reinitialize in worker process
        bootstrap_config(verbose=False)
        bootstrap_cache(verbose=False)

        # Import GPU manager
        from workers.utils.gpu_manager import GPUManager

        gpu_manager = GPUManager()
        gpu_manager.setup_for_inference()

        logger.info("âœ… Celery worker initialized successfully")

    except Exception as e:
        logger.error(f"âŒ Worker initialization failed: {e}")


@worker_shutdown.connect
def worker_shutdown_handler(sender=None, **kwargs):
    """Worker é—œé–‰è™•ç†"""
    try:
        logger.info("ğŸ›‘ Shutting down Celery worker...")

        # Cleanup GPU memory
        from workers.utils.gpu_manager import GPUManager

        gpu_manager = GPUManager()
        gpu_manager.cleanup()

        logger.info("âœ… Celery worker shutdown completed")

    except Exception as e:
        logger.error(f"âŒ Worker shutdown failed: {e}")


if __name__ == "__main__":
    celery_app.start()
