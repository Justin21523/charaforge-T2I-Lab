# tests/conftest.py
import pytest
import os
import tempfile
from fastapi.testclient import TestClient
from PIL import Image
import io

# Set test environment
os.environ["AI_CACHE_ROOT"] = tempfile.mkdtemp()
os.environ["TESTING"] = "true"

from backend.main import app


@pytest.fixture
def client():
    """FastAPI test client"""
    return TestClient(app)


@pytest.fixture
def sample_image():
    """Create a sample test image"""
    img = Image.new("RGB", (256, 256), color="red")
    img_bytes = io.BytesIO()
    img.save(img_bytes, format="PNG")
    img_bytes.seek(0)
    return img_bytes


@pytest.fixture
def sample_image_file(sample_image):
    """Sample image as file-like object"""
    return ("test.png", sample_image, "image/png")


# ===== tests/test_api/test_caption.py =====
import pytest
from httpx import AsyncClient


class TestCaptionAPI:

    def test_health_check(self, client):
        """Test API health endpoint"""
        response = client.get("/api/v1/health")
        assert response.status_code == 200
        data = response.json()
        assert data["status"] == "healthy"
        assert "cache_root" in data
        assert "gpu_available" in data

    def test_caption_endpoint_no_image(self, client):
        """Test caption endpoint without image"""
        response = client.post("/api/v1/caption")
        assert response.status_code == 422  # Validation error

    def test_caption_endpoint_invalid_file(self, client):
        """Test caption endpoint with invalid file"""
        files = {"image": ("test.txt", b"not an image", "text/plain")}
        response = client.post("/api/v1/caption", files=files)
        assert response.status_code == 400

    @pytest.mark.slow
    def test_caption_endpoint_valid_image(self, client, sample_image_file):
        """Test caption endpoint with valid image (requires model)"""
        files = {"image": sample_image_file}
        params = {"max_length": 30, "num_beams": 2}

        response = client.post("/api/v1/caption", files=files, params=params)

        if response.status_code == 200:
            data = response.json()
            assert "caption" in data
            assert "model_used" in data
            assert "elapsed_ms" in data
            assert isinstance(data["caption"], str)
            assert len(data["caption"]) > 0
        else:
            # Allow test to pass if model not available
            assert response.status_code in [500, 503]

    def test_caption_models_endpoint(self, client):
        """Test caption models list endpoint"""
        response = client.get("/api/v1/caption/models")
        assert response.status_code == 200
        data = response.json()
        assert "models" in data
        assert isinstance(data["models"], list)

    def test_caption_parameter_validation(self, client, sample_image_file):
        """Test parameter validation"""
        files = {"image": sample_image_file}

        # Test invalid max_length
        response = client.post("/api/v1/caption", files=files, params={"max_length": 5})
        assert response.status_code == 422

        # Test invalid num_beams
        response = client.post("/api/v1/caption", files=files, params={"num_beams": 0})
        assert response.status_code == 422

        # Test invalid temperature
        response = client.post(
            "/api/v1/caption", files=files, params={"temperature": 0}
        )
        assert response.status_code == 422


# ===== scripts/model_download.py =====
import os
import argparse
from pathlib import Path
from transformers import BlipProcessor, BlipForConditionalGeneration
import torch

# Shared Cache Bootstrap
AI_CACHE_ROOT = os.getenv("AI_CACHE_ROOT", "/mnt/ai_warehouse/cache")
for k, v in {
    "HF_HOME": f"{AI_CACHE_ROOT}/hf",
    "TRANSFORMERS_CACHE": f"{AI_CACHE_ROOT}/hf/transformers",
    "HF_DATASETS_CACHE": f"{AI_CACHE_ROOT}/hf/datasets",
    "HUGGINGFACE_HUB_CACHE": f"{AI_CACHE_ROOT}/hf/hub",
    "TORCH_HOME": f"{AI_CACHE_ROOT}/torch",
}.items():
    os.environ[k] = v
    Path(v).mkdir(parents=True, exist_ok=True)

# App directories
for p in [
    f"{AI_CACHE_ROOT}/models/{name}"
    for name in ["blip2", "llava", "qwen", "embeddings", "lora"]
]:
    Path(p).mkdir(parents=True, exist_ok=True)


def download_blip2():
    """Download BLIP-2 caption model"""
    print("üì• Downloading BLIP-2 caption model...")
    try:
        model_id = "Salesforce/blip-image-captioning-base"
        processor = BlipProcessor.from_pretrained(model_id)
        model = BlipForConditionalGeneration.from_pretrained(
            model_id,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        )
        print(
            f"‚úÖ BLIP-2 downloaded successfully to {os.environ['TRANSFORMERS_CACHE']}"
        )
        return True
    except Exception as e:
        print(f"‚ùå Failed to download BLIP-2: {e}")
        return False


def download_llava():
    """Download LLaVA model (placeholder)"""
    print("üì• LLaVA download - will be implemented in P2")
    return True


def download_qwen():
    """Download Qwen model (placeholder)"""
    print("üì• Qwen download - will be implemented in P3")
    return True


def download_embeddings():
    """Download embedding models (placeholder)"""
    print("üì• Embeddings download - will be implemented in P4")
    return True


def main():
    parser = argparse.ArgumentParser(description="Download AI models to shared cache")
    parser.add_argument(
        "--models",
        default="blip2",
        help="Comma-separated list of models: blip2,llava,qwen,embeddings",
    )
    parser.add_argument("--force", action="store_true", help="Force re-download")

    args = parser.parse_args()

    print(f"üöÄ Model download script")
    print(f"üìÅ Cache root: {AI_CACHE_ROOT}")
    print(f"üîß CUDA available: {torch.cuda.is_available()}")

    model_downloaders = {
        "blip2": download_blip2,
        "llava": download_llava,
        "qwen": download_qwen,
        "embeddings": download_embeddings,
    }

    models_to_download = [m.strip() for m in args.models.split(",")]
    results = {}

    for model in models_to_download:
        if model in model_downloaders:
            print(f"\nüì¶ Downloading {model}...")
            results[model] = model_downloaders[model]()
        else:
            print(f"‚ùå Unknown model: {model}")
            results[model] = False

    print("\nüìä Download Summary:")
    for model, success in results.items():
        status = "‚úÖ Success" if success else "‚ùå Failed"
        print(f"  {model}: {status}")

    print(f"\nüìÅ All models cached in: {AI_CACHE_ROOT}")


if __name__ == "__main__":
    main()

# ===== scripts/smoke_test.py =====
import requests
import sys
import time
from pathlib import Path
from PIL import Image
import io

API_BASE = "http://localhost:8000"


def test_api_health():
    """Test API health endpoint"""
    try:
        response = requests.get(f"{API_BASE}/api/v1/health", timeout=5)
        if response.status_code == 200:
            data = response.json()
            print(f"‚úÖ API Health: {data['status']}")
            print(f"   Cache: {data['cache_root']}")
            print(f"   GPU: {data['gpu_available']}")
            return True
        else:
            print(f"‚ùå Health check failed: {response.status_code}")
            return False
    except Exception as e:
        print(f"‚ùå API unavailable: {e}")
        return False


def test_caption_api():
    """Test caption API with sample image"""
    try:
        # Create test image
        img = Image.new("RGB", (256, 256), color="blue")
        img_bytes = io.BytesIO()
        img.save(img_bytes, format="PNG")
        img_bytes.seek(0)

        # Test API
        files = {"image": ("test.png", img_bytes, "image/png")}
        params = {"max_length": 30, "num_beams": 2}

        print("üß™ Testing caption API...")
        start_time = time.time()

        response = requests.post(
            f"{API_BASE}/api/v1/caption", files=files, params=params, timeout=30
        )

        elapsed = time.time() - start_time

        if response.status_code == 200:
            data = response.json()
            print(f"‚úÖ Caption generated in {elapsed:.1f}s")
            print(f"   Result: {data['caption']}")
            print(f"   Model: {data['model_used']}")
            return True
        else:
            print(f"‚ùå Caption API failed: {response.status_code}")
            print(f"   Error: {response.text}")
            return False

    except Exception as e:
        print(f"‚ùå Caption test failed: {e}")
        return False


def main():
    """Run smoke tests"""
    print("üß™ Running smoke tests for P1: Caption API")
    print("=" * 50)

    tests = [
        ("API Health", test_api_health),
        ("Caption API", test_caption_api),
    ]

    results = []
    for test_name, test_func in tests:
        print(f"\nüîç Testing: {test_name}")
        success = test_func()
        results.append((test_name, success))

        if not success:
            print(f"‚ö†Ô∏è  {test_name} failed - check if API server is running")

    print("\n" + "=" * 50)
    print("üìä Test Summary:")

    all_passed = True
    for test_name, success in results:
        status = "‚úÖ PASS" if success else "‚ùå FAIL"
        print(f"  {test_name}: {status}")
        if not success:
            all_passed = False

    if all_passed:
        print("\nüéâ All tests passed! P1 Caption API is working.")
        sys.exit(0)
    else:
        print("\n‚ö†Ô∏è  Some tests failed. Check the API server and model availability.")
        sys.exit(1)


if __name__ == "__main__":
    main()
